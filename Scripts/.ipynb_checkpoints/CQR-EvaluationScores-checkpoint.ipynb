{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55c7a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "import requests\n",
    "from pandas.io.json import json_normalize\n",
    "import json\n",
    "import os\n",
    "import os.path\n",
    "import snowflake.connector\n",
    "import boto3\n",
    "from __future__ import print_function\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import random\n",
    "\n",
    "## https://developers.google.com/sheets/api/quickstart/python\n",
    "## CREDS\n",
    "\n",
    "\n",
    "\n",
    "## In[2]:\n",
    "\n",
    "\n",
    "## https://developers.google.com/sheets/api/quickstart/python\n",
    "\n",
    "## CREDS\n",
    "# S3\n",
    "BUCKET = 'scale-crawler-enriched-csv-exports-us-west-2'\n",
    "s3 = boto3.client('s3')\n",
    "session = boto3.Session()\n",
    "\n",
    "# Google Sheets\n",
    "SCOPES = ['https://www.googleapis.com/auth/spreadsheets']\n",
    "SPREADSHEET_ID = '1ycZEbsg7hEb_kKAYmIg6eK0hBIl4fvhK0FDan1f5UkE'\n",
    "RANGE_NAME = 'Sheet9!A:M'\n",
    "PATH_TO_SECRETS_FILE = 'credentials.json'\n",
    "creds = None\n",
    "\n",
    "con = snowflake.connector.connect(user='vishal.kumar@scale.com',\n",
    "                                 account='pxa65918',\n",
    "                                 authenticator='externalbrowser',\n",
    "                                 warehouse='COMPUTE_WH',\n",
    "                                 database='SCALE_CRAWLER',\n",
    "                                 role='GENERAL_RO')\n",
    "\n",
    "cs = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3a4195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uploadData(data,filename):\n",
    "    s3.put_object(\n",
    "        ACL='bucket-owner-full-control',\n",
    "        Body=data.encode('utf-8'),\n",
    "        Bucket=RESULTS_BUCKET,\n",
    "        Key=f'flamingo_qa_potential_issues/{filename}')\n",
    "\n",
    "## Pull data from Google Sheet https://docs.google.com/spreadsheets/d/1UCIE1P6PbI9odzxFUjNF44s-SaPePbDUnHQKqxa9XpM/edit#gid=774020952\n",
    "def pullFromGS(SCOPES,PATH_TO_SECRETS_FILE,creds,SPREADSHEET_ID,RANGE_NAME):\n",
    "    if os.path.exists('token.json'):\n",
    "        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
    "\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(PATH_TO_SECRETS_FILE, SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        with open('token.json', 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "\n",
    "    try:\n",
    "        service = build('sheets', 'v4', credentials=creds)\n",
    "\n",
    "        sheet = service.spreadsheets()\n",
    "        result = sheet.values().get(spreadsheetId=SPREADSHEET_ID,range=RANGE_NAME).execute()\n",
    "        values = result.get('values', [])\n",
    "\n",
    "        if not values:\n",
    "            print('No data found.')\n",
    "        \n",
    "    except HttpError as err:\n",
    "        print(err)\n",
    "        \n",
    "    df = pd.DataFrame(values[1:],columns = values[0])    \n",
    "    return df\n",
    "\n",
    "def getCQRResults(min_date,max_date):\n",
    "    \n",
    "    sql = f'''\n",
    "    with cqr_result as (\n",
    "      with audits as (\n",
    "        select\n",
    "          sa.CATALOG_ID,\n",
    "          sa.domain,\n",
    "          sa.BODY_S3_KEY,\n",
    "          sa._id audit_id,\n",
    "          date(sa.completed_at) audit_time,\n",
    "          sa.grade :\"scores\" :\"descriptionScore\" :\"score\" as CQR_DESCRIPTION_SCORE,\n",
    "          sa.grade :\"scores\" :\"titleScore\" :\"score\" as Title,\n",
    "          sa.result\n",
    "        from\n",
    "          PUBLIC.SPOTTERAUDITS sa\n",
    "          inner join (\n",
    "            select\n",
    "              max(completed_at) as max_time,\n",
    "              CATALOG_ID\n",
    "            from\n",
    "              PUBLIC.SPOTTERAUDITS\n",
    "            group by\n",
    "              CATALOG_ID\n",
    "          ) as cqr_max on cqr_max.CATALOG_ID = sa.CATALOG_ID\n",
    "          and cqr_max.max_time = sa.completed_at\n",
    "        where\n",
    "          AUDIT_TYPE = 'Attributes'\n",
    "          and sa.COMPLETED_AT is not null\n",
    "          and sa.grade :\"scores\" :\"descriptionScore\" :\"score\" is not null\n",
    "          and date(sa.completed_at) >= '{min_date}'\n",
    "          and date(sa.completed_at) <= '{max_date}'\n",
    "      )\n",
    "      select\n",
    "        au.CATALOG_ID,\n",
    "        au.domain,\n",
    "        au.audit_id,\n",
    "        au.audit_time CQR_AUDIT_DATE,\n",
    "        au.BODY_S3_KEY,\n",
    "        a.key variant_id,\n",
    "        b.key attribute,\n",
    "        b.value :result :: string attribute_grade,\n",
    "        b.value :reason :: string reason,\n",
    "        b.value :comment :: string comment  \n",
    "      from\n",
    "        audits au,\n",
    "        lateral flatten (input => au.result) a,\n",
    "        lateral flatten (input => a.value) b\n",
    "      where\n",
    "        b.key in ('description')\n",
    "        and b.value :result = 'Incorrect'\n",
    "    )\n",
    "    select \n",
    "    c.*,\n",
    "    pv.pvid,\n",
    "    pv.scraped_attributes:link::string link\n",
    "    from cqr_result c\n",
    "    join productvariants pv on pv.unique_id = c.variant_id\n",
    "    '''\n",
    "    print('Getting CQR data from Snowflake!')\n",
    "    cs.execute(sql)\n",
    "    df = cs.fetch_pandas_all()\n",
    "    print('Success! Got CQR data from Snowflake. Number of rows:',len(df),'\\n-------------')\n",
    "    return df\n",
    "        \n",
    "def getCQRInputs(cqr_results):\n",
    "    \n",
    "    df = pd.DataFrame() \n",
    "    print('Getting CQR input data from S3!')\n",
    "    for s3_file in cqr_results['BODY_S3_KEY'].unique().tolist():\n",
    "        print('Pulling from', s3_file)\n",
    "        response = s3.get_object(Bucket = BUCKET, Key = s3_file)\n",
    "        tmp = pd.read_csv(response.get(\"Body\"))\n",
    "        df = pd.concat([df,tmp])\n",
    "    print('Success! Got CQR input data from S3. Number of rows:', len(df),'\\n-------------')\n",
    "    return df\n",
    "\n",
    "def mergeCQRData(cqr_results, cqr_inputs):\n",
    "    if len(cqr_results) == 0 or len(cqr_inputs) == 0: \n",
    "        print('ERROR: Not enough information to complete')\n",
    "        df = pd.DataFrame()\n",
    "    else:\n",
    "        print('Merging data!')\n",
    "        df = cqr_results.merge(cqr_inputs[['pvid','description','link']], left_on = 'PVID', right_on = 'pvid')\n",
    "        df = df.fillna('').rename(columns = {'description':'POST_PROCESSED_DESCRIPTION','COMMENT':'CORRECT_DESCRIPTION'})\n",
    "\n",
    "        df = df.sort_values(['POST_PROCESSED_DESCRIPTION'])\n",
    "        df = df.loc[(df['POST_PROCESSED_DESCRIPTION'] != '') & (df['CORRECT_DESCRIPTION'] != '')] \n",
    "        print('Success! Merged data. Number of rows:', len(df),'\\n-------------')\n",
    "    return df\n",
    "\n",
    "def getPPQAData(relevant_pvids):\n",
    "    pvids = \"('\" + \"','\".join(relevant_pvids) + \"')\"\n",
    "#     print(pvids)\n",
    "    sql_descs = f'''\n",
    "    select\n",
    "      user_email,\n",
    "      _ID,\n",
    "      metadata :pvids description_id,\n",
    "      b.value :: string pvid,\n",
    "      CREATED_AT variant_pped_at,\n",
    "      metadata: auditLevel :: string audit_level,\n",
    "      metadata: fieldCurrent :: string QA_DESCRIPTION\n",
    "    from\n",
    "      PUBLIC.QAEVENTS,\n",
    "      lateral flatten(input => metadata :pvids) b\n",
    "    where\n",
    "      audit_level != 'Other'\n",
    "      and METADATA :action in ('Save', 'SwitchItem')\n",
    "      and metadata: fieldCurrent is not Null\n",
    "      and pvid in {pvids}\n",
    "    '''\n",
    "\n",
    "    sql_rules = f'''\n",
    "    select\n",
    "      user_email,\n",
    "      metadata :pvids description_id,\n",
    "      b.value :: string pvid,\n",
    "      CREATED_AT variant_pped_at,\n",
    "      metadata: auditLevel :: string audit_level,\n",
    "      metadata: flagComment :: string flagtext,\n",
    "      metadata: ruleCreated :: string ruleCreated\n",
    "    from\n",
    "      PUBLIC.QAEVENTS,\n",
    "      lateral flatten(input => metadata :pvids) b\n",
    "    where\n",
    "      audit_level != 'Other'\n",
    "      and METADATA :action in ('CreateRule')\n",
    "      and metadata: flagComment is not Null\n",
    "      and pvid in {pvids}\n",
    "    '''\n",
    "    \n",
    "\n",
    "    \n",
    "    print('Getting descriptions data from Snowflake!')\n",
    "    cs.execute(sql_descs)\n",
    "    pp_desc_data = cs.fetch_pandas_all()\n",
    "    print('Success! Got descriptions data from Snowflake. Number of rows:',len(pp_desc_data))    \n",
    "    \n",
    "    print('Getting rules data from Snowflake!')\n",
    "    cs.execute(sql_rules)\n",
    "    pp_rules_data = cs.fetch_pandas_all()\n",
    "    print('Success! Got rules data from Snowflake. Number of rows:',len(pp_rules_data),'\\n-------------')    \n",
    "    \n",
    "  \n",
    "    \n",
    "    return pp_desc_data, pp_rules_data\n",
    "\n",
    "def generateSpeedAuditErrors(cqr_data, pp_desc_data):\n",
    "    cols = ['CQR_AUDIT_DATE', 'USER_EMAIL', 'type', 'AUDIT_LEVEL',\n",
    "                    'DOMAIN', 'description_PPed_at', 'sample_pvid',\n",
    "                    'sample_link','CORRECT_DESCRIPTION', 'QA_DESCRIPTION',\n",
    "                   'Extra text (not removed by QA)',\n",
    "                   'Missing text (incorrectly removed by QA)','outcome']\n",
    "        \n",
    "    if len(cqr_data) == 0 or len(pp_desc_data) == 0: \n",
    "        print('ERROR: Not enough information to complete')\n",
    "        dff = pd.DataFrame(columns = cols)\n",
    "    else: \n",
    "        print('Generating Speed Audit errors!')\n",
    "        df = cqr_data.merge(pp_desc_data, on = 'PVID')\n",
    "        df = df.rename(columns = {'COMMENT':'CORRECT_DESCRIPTION'})\n",
    "        df['clean_final_desc'] = df.apply(lambda x: re.sub('\\\\\\\\n|\\n| ','',x['CORRECT_DESCRIPTION']),axis=1)\n",
    "        df['clean_fieldcurrent'] = df.apply(lambda x: re.sub('\\\\\\\\n|\\n| ','',x['QA_DESCRIPTION']),axis=1)\n",
    "        df['is_correct_desc'] = df['clean_final_desc'] == df['clean_fieldcurrent']\n",
    "        df = df.drop_duplicates() # .loc[df['is_correct_desc'] == False]\n",
    "        if len(df) ==0:\n",
    "            return df\n",
    "        else:\n",
    "            tmp_cols = ['CQR_AUDIT_DATE',\n",
    "                'USER_EMAIL',\n",
    "                'AUDIT_LEVEL',\n",
    "                'DOMAIN',\n",
    "                'CORRECT_DESCRIPTION',\n",
    "                'QA_DESCRIPTION','is_correct_desc']\n",
    "\n",
    "            dff = df.groupby(tmp_cols)['VARIANT_PPED_AT','PVID','LINK'].min()                .reset_index()                .rename(columns = {'VARIANT_PPED_AT':'description_PPed_at','PVID':'sample_pvid','LINK':'sample_link'})\n",
    "            dff['Extra text (not removed by QA)'] = dff.apply(lambda x: np.setdiff1d([i.strip('. ').strip('! ').strip('? ').lower() for i in re.split('\\. |\\n|\\! |\\? ', x['QA_DESCRIPTION']) if i != ''],[i.strip('. ').strip('! ').strip('? ').lower() for i in re.split('\\. |\\n|\\! |\\? ', x['CORRECT_DESCRIPTION']) if i != '']), axis = 1)    \n",
    "            dff['Missing text (incorrectly removed by QA)'] = dff.apply(lambda x: np.setdiff1d([i.strip('. ').strip('! ').strip('? ').lower() for i in re.split('\\. |\\n|\\! |\\? ', x['CORRECT_DESCRIPTION']) if i != ''],[i.strip('. ').strip('! ').strip('? ').lower() for i in re.split('\\. |\\n|\\! |\\? ', x['QA_DESCRIPTION']) if i != '']), axis = 1)\n",
    "            dff['type'] = 'Speed Audit'\n",
    "            dff['outcome'] = dff.apply(lambda x: 'incorrect speed audit' if x['is_correct_desc'] == False else 'correct speed audit', axis = 1)\n",
    "           \n",
    "            print('Success! Generated Speed Audit Errors\\n-------------')     \n",
    "        return dff.loc[:,cols]\n",
    "\n",
    "def generateFlagAuditErrors(full_cqr_data, pp_rules_data):\n",
    "    cols = ['CQR_AUDIT_DATE', 'USER_EMAIL','type','AUDIT_LEVEL',\n",
    "                'DOMAIN', 'description_PPed_at', 'sample_pvid',\n",
    "                'sample_link','FLAGTEXT', 'RULECREATED',\n",
    "               'Extra text (not removed by QA)',\n",
    "               'Missing text (incorrectly removed by QA)','outcome']\n",
    "    if len(full_cqr_data) == 0 or len(pp_rules_data) == 0: \n",
    "        print('ERROR: Not enough information to complete')\n",
    "        dff = pd.DataFrame(columns = cols)\n",
    "    else: \n",
    "        print('Generating Flag Audit errors!')\n",
    "        df = full_cqr_data.merge(pp_rules_data, on = 'PVID')\n",
    "        df = df.rename(columns = {'COMMENT':'CORRECT_DESCRIPTION'})\n",
    "        cols = ['CQR_AUDIT_DATE',\n",
    "            'USER_EMAIL',\n",
    "            'AUDIT_LEVEL',\n",
    "            'DOMAIN',\n",
    "            'POST_PROCESSED_DESCRIPTION',\n",
    "            'CORRECT_DESCRIPTION',\n",
    "               'FLAGTEXT','RULECREATED']\n",
    "        dff = df.groupby(cols)['VARIANT_PPED_AT','PVID','LINK'].min()            .reset_index()            .rename(columns = {'VARIANT_PPED_AT':'description_PPed_at','PVID':'sample_pvid','LINK':'sample_link'})\n",
    "        dff['Extra text (not removed by QA)'] = dff.apply(lambda x: np.setdiff1d([i.strip('. ') for i in re.split('\\. |\\n|\\! |\\? ', x['POST_PROCESSED_DESCRIPTION']) if i != ''],[i.strip('. ') for i in re.split('\\. |\\n|\\! |\\? ', x['CORRECT_DESCRIPTION']) if i != '']), axis = 1)    \n",
    "        dff['Missing text (incorrectly removed by QA)'] = dff.apply(lambda x: np.setdiff1d([i.strip('. ') for i in re.split('\\. |\\n|\\! |\\? ', x['CORRECT_DESCRIPTION']) if i != ''],[i.strip('. ') for i in re.split('\\. |\\n|\\! |\\? ', x['POST_PROCESSED_DESCRIPTION']) if i != '']), axis = 1)\n",
    "\n",
    "        dff['bad_removal'] = dff.apply(lambda x: x['RULECREATED'] == 'true' and re.sub(\"\\.|\\'|\\,\",'',x['FLAGTEXT'].strip().lower()) in re.sub(\"\\.|\\'|\\,\",'',str(x['Missing text (incorrectly removed by QA)']).strip().lower()),axis = 1)\n",
    "        dff['bad_inclusion'] = dff.apply(lambda x:  x['RULECREATED'] == 'false' and re.sub(\"\\.|\\'|\\,\",'',x['FLAGTEXT'].strip().lower()) in re.sub(\"\\.|\\'|\\,\",'',str(x['Extra text (not removed by QA)']).strip().lower()),axis = 1)\n",
    "        dff['outcome'] = dff.apply(lambda x: 'bad flag removal' if x['bad_removal'] == True else ('bad flag inclusion' if x['bad_inclusion'] == True else 'ok'), axis = 1)\n",
    "        dff['type'] = 'Flag Audit'\n",
    "\n",
    "        print('Success! Generated Flag Audit Errors\\n-------------')    \n",
    "    return dff.loc[:,cols] #dff['outcome'] != 'ok',\n",
    "\n",
    "def completeErrorReport(speed_audit_errors,flag_audit_errors):\n",
    "    df = pd.concat([speed_audit_errors,flag_audit_errors])[['CQR_AUDIT_DATE',\n",
    "    'USER_EMAIL',\n",
    "    'type',\n",
    "    'AUDIT_LEVEL',\n",
    "    'DOMAIN',\n",
    "    'description_PPed_at',\n",
    "    'sample_pvid',\n",
    "    'sample_link',\n",
    "    'CORRECT_DESCRIPTION',\n",
    "    'QA_DESCRIPTION',\n",
    "    'FLAGTEXT',\n",
    "    'RULECREATED',\n",
    "    'Extra text (not removed by QA)',\n",
    "    'Missing text (incorrectly removed by QA)',\n",
    "    'outcome']]\n",
    "    df = df.sort_values(['DOMAIN','USER_EMAIL'])\n",
    "    \n",
    "    print(df.shape)\n",
    "    \n",
    "    df['CQR_AUDIT_DATE'] = pd.to_datetime(df['CQR_AUDIT_DATE'],utc=True)\n",
    "    df['description_PPed_at'] = pd.to_datetime(df['description_PPed_at'],utc=True)\n",
    "    df = df.loc[abs((df['CQR_AUDIT_DATE'] - df['description_PPed_at']).dt.days) <= 7] ## only include work done in past week\n",
    "    \n",
    "    df.loc[df['outcome'].isin(['incorrect speed audit','bad flag removal'])].to_clipboard(index = False)\n",
    "    print('Error Report created!')\n",
    "    return df\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8179d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPPIDData(relevant_pvids):\n",
    "    pvids = \"('\" + \"','\".join(relevant_pvids) + \"')\"\n",
    "\n",
    "sql_ids =f'''\n",
    "    with fb as (\n",
    "  SELECT\n",
    "    qe._id as ID,\n",
    "    qe.user_email :: string uemail,\n",
    "    qe.metadata: action action,\n",
    "    qe.metadata: auditField auditField,\n",
    "    qe.metadata: domain domain,\n",
    "    qe.metadata: eventCreatedAt eventCreatedAt,\n",
    "    qe.metadata: fieldAdditions fieldAdditions,\n",
    "    qe.metadata: fieldBefore fieldBefore,\n",
    "    qe.metadata: fieldCurrent fieldCurrent,\n",
    "    qe.metadata: fieldLength fieldLength,\n",
    "    qe.metadata: ruleCreated ruleCreated,\n",
    "    qe.metadata: fieldRemovals fieldRemovals,\n",
    "    qe.metadata: flagsAvailable flagsAvailable,\n",
    "    qe.metadata: hintsAvailable hintsAvailable,\n",
    "--  qe.metadata: pvids pvids,\n",
    "    qe.metadata: totalMillisElapsed totalMillisElapsed,\n",
    "    qe.event_type,\n",
    "    qe.event_at,\n",
    "    cast (qe.event_at as DATE) AS EventDate,\n",
    "    qe.metadata: auditLevel :: string auditLevel,\n",
    "    pv.value:: string as pvid\n",
    "  FROM\n",
    "    PUBLIC.QAEVENTS qe,\n",
    "    lateral flatten(input => qe.metadata: pvids) pv\n",
    "--    lateral flatten(input => pv.value, outer => true) pvd\n",
    "   -- AND qe.event_type = 'submit_description_speed_audit' --  AND qe.metadata: auditLevel !='QA'\n",
    "    -- AND qe.user_email LIKE '%tele%'\n",
    ")\n",
    "select\n",
    "  fb.ID,\n",
    "  fb.uemail,\n",
    "  fb.action,\n",
    "  fb.auditField,\n",
    "  fb.domain,\n",
    "  fb.eventCreatedAt,\n",
    "  fb.fieldAdditions,\n",
    "  fb.fieldBefore,\n",
    "  fb.fieldCurrent,\n",
    "  fb.fieldLength,\n",
    "  fb.fieldRemovals,\n",
    "  fb.ruleCreated,\n",
    "  fb.flagsAvailable,\n",
    "  fb.hintsAvailable,\n",
    "  fb.pvid,\n",
    "  fb.totalMillisElapsed,\n",
    "  fb.event_type,\n",
    "  fb.event_at,\n",
    "  fb.EventDate\n",
    "from\n",
    "  fb\n",
    "WHERE fb.pvid IN {pvids}\n",
    "    '''\n",
    "\n",
    "print('Getting error IDs data from Snowflake!')\n",
    "cs.execute(sql_ids)\n",
    "pp_ids_data = cs.fetch_pandas_all()\n",
    "print('Success! Got all error IDs data from Snowflake. Number of rows:',len(pp_ids_data),'\\n-------------') \n",
    "return pp_ids_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117f4901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[4]:\n",
    "\n",
    "\n",
    "date_in = '09/16/2022'\n",
    "# date_out = date_in\n",
    "date_out = '09/18/2022'\n",
    "\n",
    "print(f'ERROR LOGS {date_in} to {date_out}\\n')\n",
    "cqr_results = getCQRResults(date_in,date_out)\n",
    "cqr_inputs = getCQRInputs(cqr_results)\n",
    "#full_cqr_data = mergeCQRData(cqr_results, cqr_inputs)\n",
    "#pp_desc_data, pp_rules_data = getPPQAData(cqr_results['PVID'].unique().tolist())\n",
    "pp_ids_data=getPPIDData(cqr_results['PVID'].unique().tolist())\n",
    "speed_audit_errors = generateSpeedAuditErrors(cqr_results, pp_desc_data)\n",
    "flag_audit_errors = generateFlagAuditErrors(full_cqr_data, pp_rules_data)\n",
    "df = completeErrorReport(speed_audit_errors,flag_audit_errors)\n",
    "#df.to_csv('fullerrorreport.csv')\n",
    "\n",
    "\n",
    "# In[6]: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4214c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_clipboard(index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce99f7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_ids_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4203abcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_desc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51df33d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
