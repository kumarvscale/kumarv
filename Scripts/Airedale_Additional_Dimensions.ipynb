{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating login request with your identity provider. A browser window should have opened for you to complete the login. If you can't see it, check existing browser windows, or your OS settings. Press CTRL+C to abort and try again...\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import snowflake.connector\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "api_key_file = 'apikey.txt'\n",
    "if os.path.isfile(api_key_file):\n",
    "    with open(api_key_file) as f:\n",
    "        openai.api_key = f.readline()\n",
    "else:\n",
    "    print(f\"Error: {api_key_file} not found.\")\n",
    "\n",
    "OPENAI_API_KEY = openai.api_key\n",
    "\n",
    "#login to snowflake db\n",
    "con = snowflake.connector.connect(user='vishal.kumar@scale.com',\n",
    "                                 account='pxa65918',\n",
    "                                 authenticator='externalbrowser',\n",
    "                                 warehouse='COMPUTE_WH',\n",
    "                                 database='SCALE_CRAWLER',\n",
    "                                 role='GENERAL_RO')\n",
    "\n",
    "cs = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data from snowflake\n",
    "sql = f'''\n",
    "select \n",
    "    t._id as task_id,\n",
    "    t.params:templateVariables:Domain::string as DOMAIN,\n",
    "    t.params:templateVariables:Subtopic::string as SUB_TOPIC,\n",
    "    t.params:templateVariables:Category::string as CATEGORY,\n",
    "    t.params:templateVariables:Guidance::string as GUIDANCE,\n",
    "    ta._ID as EVALUATION,\n",
    "    ta.attempted_by as EVALUATOR,\n",
    "    t.response:consensusMeta:consensusResponse:responses[3]:output::string as PROMPT,\n",
    "    t.response:consensusMeta:consensusResponse:responses[3]:context:referenceTexts[0]:content::string as REF_TEXT,\n",
    "    t.response:consensusMeta:consensusResponse:responses[3]:context:referenceTexts[0]:url::string as REF_URL,\n",
    "    case\n",
    "    when (t.response:consensusMeta:consensusResponse:responses[4]:context:displayOrder[0]) = 0 then t.response:consensusMeta:consensusResponse:responses[4]:context:candidates[0]:message:content\n",
    "    when (t.response:consensusMeta:consensusResponse:responses[4]:context:displayOrder[1]) = 0 then t.response:consensusMeta:consensusResponse:responses[4]:context:candidates[1]:message:content    \n",
    "    else 'NA'\n",
    "    end as AI21_RESPONSE,\n",
    "    case\n",
    "    when (t.response:consensusMeta:consensusResponse:responses[4]:context:displayOrder[0]) = 1 then t.response:consensusMeta:consensusResponse:responses[4]:context:candidates[0]:message:content\n",
    "    when (t.response:consensusMeta:consensusResponse:responses[4]:context:displayOrder[1]) = 1 then t.response:consensusMeta:consensusResponse:responses[4]:context:candidates[1]:message:content    \n",
    "    else 'NA'\n",
    "    end as GPT4_RESPONSE,\n",
    "    ta.response:responses[6].context.selectedId::string as PREFERRED_MODEL,\n",
    "    ta.response:responses[6].context.annotations.Airdale.annotations.response_factuality.response[0]::string as AI21_RESPONSE_FACTUALITY,\n",
    "    ta.response:responses[6].context.annotations.Airdale.annotations.instruction_following.response[0]::string as AI21_INSTRUCTION_FOLLOWING,\n",
    "    ta.response:responses[6].context.annotations.Airdale.annotations.Style.response[0]::string as AI21_STYLE,\n",
    "    ta.response:responses[6].context.annotations.Airdale.annotations.Overall.response[0]::string as AI21_OVERALL,\n",
    "    ta.response:responses[6].context.annotations.OpenAI.annotations.response_factuality.response[0]::string as OpenAI_RESPONSE_FACTUALITY,\n",
    "    ta.response:responses[6].context.annotations.OpenAI.annotations.instruction_following.response[0]::string as OpenAI_INSTRUCTION_FOLLOWING,\n",
    "    ta.response:responses[6].context.annotations.OpenAI.annotations.Style.response[0]::string as OpenAI_STYLE,\n",
    "    ta.response:responses[6].context.annotations.OpenAI.annotations.Overall.response[0]::string as OpenAI_OVERALL,\n",
    "    ta.response:responses[8].context.response.annotations.model_feedback.response[0]::string as EVALUATION_MODEL_FEEDBACK,\n",
    "    SUBSTRING(\n",
    "        t.params:before[3].params:instructions::string, \n",
    "        CHARINDEX('**Number of Instructions:**', t.params:before[3].params:instructions::string) + LEN('**Number of Instructions:**'), \n",
    "        CHARINDEX('**Main Topic:**', t.params:before[3].params:instructions::string) - CHARINDEX('**Number of Instructions:**', t.params:before[3].params:instructions::string) - LEN('**Number of Instructions:**')\n",
    "    ) as NUMBER_OF_INSTRUCTIONS\n",
    "from scale_prod.public.tasks t\n",
    "join scale_prod.public_w_deleted.taskattempts ta on t._id = ta.task\n",
    "where t.batch='65e347e8b47aa6d0d22eb2a3'\n",
    "and attempted_at_review_level = 4\n",
    "order by t._id\n",
    "'''\n",
    "cs.execute(sql)\n",
    "idf = cs.fetch_pandas_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add 4 additional columns to idf called NUMBER_OF_INSTRUCTIONS, LENGTH_INSTRUCTIONS, FORMAT_TYPE\n",
    "idf['NUMBER_OF_INSTRUCTIONS'] = ''\n",
    "idf['LENGTH_INSTRUCTIONS'] = ''\n",
    "idf['FORMAT_TYPE'] = ''\n",
    "\n",
    "#remove duplicate task_id from idf\n",
    "idf = idf.drop_duplicates(subset=['TASK_ID'])\n",
    "\n",
    "def evaluator_gpt(prompt):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4-turbo-preview\",\n",
    "        messages=[{\"role\":\"user\", \"content\":prompt}],\n",
    "        temperature=0.1\n",
    "    )\n",
    "    response = completion.choices[0].message\n",
    "    response = response.content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [19:16<00:00,  2.09it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  \n",
    "idf['NUMBER_OF_INSTRUCTIONS'] = None\n",
    "for i, row in tqdm(idf.iterrows(), total=idf.shape[0]):\n",
    "    try:\n",
    "        prompt = row['PROMPT']\n",
    "        number_of_instructions_eval_query = f\"You are tasked with counting the number of instructions in a certain provided question. Respond with the numeric value of number of instructions only. Here is the question \\n{prompt}\"\n",
    "        response = evaluator_gpt(number_of_instructions_eval_query)\n",
    "        idf.at[i, 'NUMBER_OF_INSTRUCTIONS'] = response\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred for index {i}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [21:42<00:00,  1.85it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  \n",
    "idf['FORMAT_TYPE'] = None\n",
    "for i, row in tqdm(idf.iterrows(), total=idf.shape[0]):\n",
    "    try:\n",
    "        prompt = row['PROMPT']\n",
    "        format_eval_query = f\"You are now tasked with finding the format type requested in a question. Format type examples are: bullet list, numbered list, table markdown, comma separated list, general markdown etc. You need to respond with only the format and nothing else. Here is the question \\n{prompt}\"\n",
    "        response = evaluator_gpt(format_eval_query)\n",
    "        idf.at[i, 'FORMAT_TYPE'] = response\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred for index {i}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [22:46<00:00,  1.77it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  \n",
    "idf['LENGTH_INSTRUCTIONS'] = None\n",
    "for i, row in tqdm(idf.iterrows(), total=idf.shape[0]):\n",
    "    try:\n",
    "        prompt = row['PROMPT']\n",
    "        length_eval_query = f\"You are now tasked with finding the length instructions in a question. Length instruction is something that specifies how long the answer needs to be. \\nFor word length specification respond: <num of words>,words\\nFor paragraph specification respond: <num of paragraphs>, paragraphs\\nIf no length instruction exists, respond with 'None'. You need to respond with only with the numeric length and length instruction type as shown above. Here is the question:\\n{prompt}\"\n",
    "        response = evaluator_gpt(length_eval_query)\n",
    "        idf.at[i, 'LENGTH_INSTRUCTIONS'] = response\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred for index {i}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save idf to csv\n",
    "idf.to_csv('Airedale.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
