{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating login request with your identity provider. A browser window should have opened for you to complete the login. If you can't see it, check existing browser windows, or your OS settings. Press CTRL+C to abort and try again...\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "from llmengine import Completion\n",
    "from pyjarowinkler import distance\n",
    "\n",
    "api_key_file = '/Users/vishalkumar/Documents/apikey.txt'\n",
    "if os.path.isfile(api_key_file):\n",
    "    with open(api_key_file) as f:\n",
    "        openai.api_key = f.readline()\n",
    "else:\n",
    "    print(f\"Error: {api_key_file} not found.\")\n",
    "\n",
    "OPENAI_API_KEY = openai.api_key\n",
    "\n",
    "#login to snowflake db\n",
    "con = snowflake.connector.connect(user='vishal.kumar@scale.com',\n",
    "                                 account='pxa65918',\n",
    "                                 authenticator='externalbrowser',\n",
    "                                 warehouse='COMPUTE_WH',\n",
    "                                 database='SCALE_CRAWLER',\n",
    "                                 role='GENERAL_RO')\n",
    "\n",
    "cs = con.cursor()\n",
    "\n",
    "\n",
    "def evaluator_gpt(prompt):\n",
    "    import openai\n",
    "    client = openai\n",
    "    text_response = client.chat.completions.create(\n",
    "        model=\"gpt-4-0125-preview\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=512\n",
    "    )\n",
    "    return text_response\n",
    "\n",
    "\n",
    "def llama_fix(prompt):\n",
    "    response = Completion.create(\n",
    "        model=\"llama-3-70b-instruct\",\n",
    "        prompt=prompt,\n",
    "        max_new_tokens=1024,\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    return response.output.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating login request with your identity provider. A browser window should have opened for you to complete the login. If you can't see it, check existing browser windows, or your OS settings. Press CTRL+C to abort and try again...\n",
      "Index(['PROMPT_TASK_ID', 'TASK_ID', 'REVIEW_LEVEL', 'ATTEMPT', 'BATCH',\n",
      "       'CATEGORY', 'SUBCATEGORY', 'AB_RATIO', 'TURN', 'PROMPT_TYPE', 'PROMPT',\n",
      "       'RESPONSE', 'EDITED_RESPONSE', 'SELECTED'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Get data from snowflake\n",
    "sql = f'''\n",
    "with task_list as (\n",
    "     select --ta.task,\n",
    "     ta._id attempt\n",
    "     ,row_number() over (partition by ta.task order by ta.attempted_at desc) as rn\n",
    "     from scale_prod.public.taskattempts ta\n",
    "     where ta.project = '66352fe26aa48684b7379b96'\n",
    "     --and ta.task = '663e4b7dee7efc37538de585'\n",
    "    )\n",
    "\n",
    ",base as (\n",
    " select\n",
    "    --> Attempt\n",
    "    t._id AS task_id,\n",
    "    ta._id attempt,\n",
    "    tb.name batch,\n",
    "    t.params:turn[0]:params:chat_models[0]:url::string as model,\n",
    "    t.params :templateVariables :Subcategory :: string as Subcategory,\n",
    "    t.params :templateVariables :Category :: string as Category,\n",
    "    t.params :templateVariables :AB_ratio :: string as AB_ratio,\n",
    "    t.params :templateVariables :taskID :: string as Prompt_task_id,\n",
    "    t.params :templateVariables :PROMPTTYPE :: string as prompt_type,\n",
    "    t.TASK_TEMPLATE_VERSION,\n",
    "    v3.review_level\n",
    "    \n",
    "    \n",
    "    ,r.value:type::string as step\n",
    "    ,r.value:index as index\n",
    "    \n",
    "    ,CASE WHEN r.value:type = 'PromptInput' THEN r.value:output::string  \n",
    "    WHEN r.value:type = 'MultiTurnContinue' THEN r.value:output::string ELSE NULL END AS prompt\n",
    "    \n",
    "    ,case when r.value:type = 'ModelResponseSelector' then r.value:output::string else null end as response\n",
    "    ,case when r.value:type = 'ModelResponseEditor' then r.value:output::string else null end as edited_response\n",
    "    \n",
    "    ,r.value:context:selectedId::string as selected\n",
    "    \n",
    "    \n",
    "FROM\n",
    "    scale_prod.public.tasks t\n",
    "    LEFT JOIN scale_prod.public.taskattempts ta ON ta.task = t._id \n",
    "    INNER JOIN task_list tl on ta._id = tl.attempt and rn = 1\n",
    "    LEFT JOIN scale_prod.public.taskbatches tb on t.batch = tb._id\n",
    "    LEFT JOIN scale_prod.public.PIPELINEV3HUMANNODES v3 on tl.attempt = v3.attempt_to_review and v3.status = 'pending'\n",
    "   \n",
    "    ,LATERAL FLATTEN (input => ta.response:responses, mode => 'array') r\n",
    "   \n",
    "WHERE\n",
    "    t.project = '66352fe26aa48684b7379b96'\n",
    "    AND v3.review_level in (0,1,8)\n",
    "   -- AND v3.status = 'completed'\n",
    "    AND t.status = 'pending'\n",
    "    AND step not in ('Instruction')\n",
    "    --AND t._id = '663e4b7fe78ea72f97aa7ca8'\n",
    "    And v3.status != 'paused'\n",
    "    \n",
    ")\n",
    "\n",
    ",prompts as (\n",
    "        select Prompt_task_id, review_level, task_id, attempt,TASK_TEMPLATE_VERSION, batch, Category, Subcategory, AB_ratio,\n",
    "        row_number() over (partition by task_id order by index asc) as turn,\n",
    "        prompt,\n",
    "        prompt_type\n",
    "        from base\n",
    "        where step in ('PromptInput', 'MultiTurnContinue')\n",
    "        and prompt != ''\n",
    "        order by task_id desc, turn asc\n",
    "    )\n",
    "\n",
    ",responses as (\n",
    "        select task_id, response, selected,\n",
    "        row_number() over (partition by task_id order by index asc) as rn\n",
    "        from base\n",
    "        where step in ('ModelResponseSelector')\n",
    "        and response != ''\n",
    "    )\n",
    "    \n",
    ",edited as (\n",
    "        select task_id, edited_response, row_number() over (partition by task_id order by index asc) as rn\n",
    "        from base\n",
    "        where step in ('ModelResponseEditor')\n",
    "        and edited_response is not null\n",
    "    )\n",
    "  \n",
    "select \n",
    "p.Prompt_task_id,\n",
    "p.task_id,\n",
    "p.review_level,\n",
    "p.attempt,\n",
    "--p.TASK_TEMPLATE_VERSION,\n",
    "p.batch,\n",
    "p.category,\n",
    "p.subcategory,\n",
    "p.ab_ratio,\n",
    "p.turn,\n",
    "p.prompt_type,\n",
    "p.prompt,\n",
    "r.response,\n",
    "e.edited_response,\n",
    "r.selected\n",
    "\n",
    "--,div0(EDITDISTANCE(r.response, e.edited_response), GREATEST(LENGTH(r.response), LENGTH(e.edited_response))) *100 as edits\n",
    "\n",
    "from prompts p\n",
    "inner join responses r on p.task_id = r.task_id and p.turn = r.rn\n",
    "inner join edited e on p.task_id = e.task_id and p.turn = e.rn\n",
    "where 1=1 \n",
    "--and p.TASK_TEMPLATE_VERSION >= 37\n",
    "order by p.task_id desc, turn asc \n",
    "'''\n",
    "cs.execute(sql)\n",
    "idf = cs.fetch_pandas_all()\n",
    "print(idf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 14/8890 [00:39<6:59:09,  2.83s/it]"
     ]
    }
   ],
   "source": [
    "prompt_concise_qc = \"Analyze the text given for conciseness. If you find repetitive or unnecessary information which not critical in the meaning then say FAIL, otherwise if the text is non-repetitive say PASS. Here is the text:\\n\\n\"\n",
    "prompt_refusal_qc = \"Analyse the text and tell if it is primarily a direct refusal to answer then respond with text 'REFUSAL' otherwise respond with 'NOT REFUSAL'. Here is the text:\\n\\n\"\n",
    "prompt_judge_qc = \"Analyse the text given and tell if it has a very judgemental tone. If you find the text to be very judgemental then respond with 'JUDGEMENTAL' otherwise respond with 'NOT JUDGEMENTAL'. Here is the text:\\n\\n\"\n",
    "\n",
    "restricted_phrases = ['I’m deeply concerned','Whoa there','I cannot satisfy your request','I am sorry','let’s keep it PG','Whoa there','Can I help you','It’s inappropriate to','It is inappropriate to','I cannot','I’m sorry','As a helpful assistant','As an AI','I am programmed to','As a model','As a AI','I apologize','I would like to emphasize','In summary','Illegal or unethical','Illegal and unethical','I am unable to','I am sorry','It’s essential to','I understand']\n",
    "rest_count = 0\n",
    "\n",
    "\n",
    "for index, row in tqdm(idf.iterrows(), total=idf.shape[0]):\n",
    "    text =  row['EDITED_RESPONSE']\n",
    "    #check for restricted phrases in text\n",
    "    for phrase in restricted_phrases:\n",
    "        if phrase in text:\n",
    "            #print(f\"Restricted phrase found in response: {phrase}\",text)\n",
    "            rest_count = rest_count + 1\n",
    "        #save rest_count to a new column called 'RESTRICTED_PHRASES'\n",
    "    idf.loc[index, 'RESTRICTED_PHRASES'] = rest_count\n",
    "\n",
    "    text = row['EDITED_RESPONSE']\n",
    "    text = prompt_concise_qc + text\n",
    "    response = evaluator_gpt(text)\n",
    "    response = response.choices[0].message.content \n",
    "    #save response to a new column called 'CONCISE_QC'\n",
    "    idf.loc[index, 'CONCISE_QC'] = response\n",
    "\n",
    "    text = row['EDITED_RESPONSE']\n",
    "    text = prompt_refusal_qc + text\n",
    "    response = evaluator_gpt(text)\n",
    "    response = response.choices[0].message.content \n",
    "    #print(row['EDITED_RESPONSE'],response)\n",
    "    idf.loc[index, 'REFUSAL_QC'] = response\n",
    "\n",
    "    text = row['EDITED_RESPONSE']\n",
    "    text = prompt_judge_qc + text\n",
    "    response = evaluator_gpt(text)\n",
    "    response = response.choices[0].message.content \n",
    "    #print(row['EDITED_RESPONSE'],response)\n",
    "    idf.loc[index, 'JUDGEMENTAL_QC'] = response\n",
    "\n",
    "    #check if response ends in a ?\n",
    "    text = row['EDITED_RESPONSE']\n",
    "    if text.endswith('?'):\n",
    "        idf.loc[index, 'ENDS_WITH_QUESTION'] = \"YES\"\n",
    "    else:\n",
    "        idf.loc[index, 'ENDS_WITH_QUESTION'] = \"NO\"\n",
    "\n",
    "    #check the jarowinkler similarity between RESPONSE and EDITED_RESPONSE\n",
    "    text1 = row['RESPONSE']\n",
    "    text2 = row['EDITED_RESPONSE']\n",
    "    from pyjarowinkler import distance\n",
    "    jw = distance.get_jaro_distance(text1, text2, winkler=True, scaling=0.1)\n",
    "    idf.loc[index, 'JAROWINKLER'] = jw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf.to_csv('Flamingo_Safety_Auto_QA.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
