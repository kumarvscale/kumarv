{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating login request with your identity provider. A browser window should have opened for you to complete the login. If you can't see it, check existing browser windows, or your OS settings. Press CTRL+C to abort and try again...\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import snowflake.connector\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "api_key_file = 'apikey.txt'\n",
    "if os.path.isfile(api_key_file):\n",
    "    with open(api_key_file) as f:\n",
    "        openai.api_key = f.readline()\n",
    "else:\n",
    "    print(f\"Error: {api_key_file} not found.\")\n",
    "\n",
    "OPENAI_API_KEY = openai.api_key\n",
    "\n",
    "#login to snowflake db\n",
    "con = snowflake.connector.connect(user='vishal.kumar@scale.com',\n",
    "                                 account='pxa65918',\n",
    "                                 authenticator='externalbrowser',\n",
    "                                 warehouse='COMPUTE_WH',\n",
    "                                 database='SCALE_CRAWLER',\n",
    "                                 role='GENERAL_RO')\n",
    "\n",
    "cs = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the prompt and response data from the snowflake db\n",
    "sql = f'''\n",
    "select\n",
    "  ta.task,\n",
    "  ta.attempted_by,\n",
    "  t.params:templateVariables:Domain::string as Domain,\n",
    "  t.params:templateVariables:Subtopic::string as Subtopic,\n",
    "  t.params:templateVariables:Category::string as Category,\n",
    "  t.params:templateVariables:Subcategory::string as Subcategory,\n",
    "  t.params:templateVariables:Subcategory_Description::string as Subcategory_Description,\n",
    "  ta.response :responses [0] :output :: string as Prompt,\n",
    "  ta.response :rewrite :: string as Response\n",
    "from\n",
    "  scale_prod.public.taskattempts ta\n",
    "  join scale_prod.public.tasks t on ta.task = t._id\n",
    "where\n",
    "  ta.project = '65c6b841652f25eabae60e72'\n",
    "limit\n",
    "  10\n",
    "'''\n",
    "cs.execute(sql)\n",
    "idf = cs.fetch_pandas_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator_gpt(prompt):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4-turbo-preview\",\n",
    "        messages=[{\"role\":\"user\", \"content\":prompt}],\n",
    "        temperature=0.1\n",
    "    )\n",
    "    response = completion.choices[0].message\n",
    "    response = response.content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:16<00:00,  1.68s/it]\n"
     ]
    }
   ],
   "source": [
    "# add three new columns to the dataframe BINARY_EVAL, QUAL_EVAL and IMPROVED_PROMPT\n",
    "idf['BINARY_EVAL'] = ''\n",
    "idf['QUAL_EVAL'] = ''\n",
    "idf['IMPROVED_PROMPT'] = ''\n",
    "#start a for loop to iterate through idf, add tqdm to show progress\n",
    "for i in tqdm(range(len(idf))):\n",
    "    #get the domain, category, subcategory and subcategory description from the first row of the dataframe\n",
    "    domain = idf['DOMAIN'][i]\n",
    "    category = idf['CATEGORY'][i]\n",
    "    subcategory = idf['SUBCATEGORY'][i]\n",
    "    subcategory_description = idf['SUBCATEGORY_DESCRIPTION'][i]\n",
    "    prompt = idf['PROMPT'][i]\n",
    "    eval_query=\"You are given a prompt(question) and you need to check whether it follows various criteria. Here are the criterion you need to check for\\n\\nDomain\\n Check whether the question belongs to this domain or not. \\nCategory\\n Check whether the question belongs to this category or not\\nSubcategory\\n Check whether the question belongs to this category or not based on a subcategory description which will be provided to you. \\n\\nHere is the question and the criteria that needs your assessment\\n\\n\" + prompt + \"\\nDomain:\" + domain + \"\\nCategory:\" + category + \"\\nSubcategory:\" + subcategory + \" (\" + subcategory_description + \")\\nRespond in following format:\\nDomain: Yes/No\\nCategory: Yes/No\\nSubcategory: Yes/No\\n\\nDont say anything else, just stick to the format above\"\n",
    "    response = evaluator_gpt(eval_query)\n",
    "    #add the response to the dataframe in BINARY_EVAL column\n",
    "    idf['BINARY_EVAL'][i] = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:12<00:00,  1.21s/it]\n"
     ]
    }
   ],
   "source": [
    "#start qualitiative evaluation\n",
    "for i in tqdm(range(len(idf))):\n",
    "    #get the domain, category, subcategory and subcategory description from the first row of the dataframe\n",
    "    prompt = idf['PROMPT'][i]\n",
    "    eval_query=\"You are now tasked with providing additional insights about the question. \\nCreativity: Is the question creative. A creative question is something that is not commonly found on the internet. It is original in its ask and requires multiple levels of thought to answer. \\nDepth: Does the question go into the depth of a topic, or is it superficial and simple. \\nNumber of instructions: How many instructions are in the question. \\n\\nHere is the question that needs your assessment:\" + prompt + \"\\nRespond in following format:\\nCreativity: Very-Low/Low/Medium/High\\nDepth: Very-Low/Low/Medium/High\\nNumber of instructions: 1/2/3/4/5/6/7/8/9/10 etc\\n\\nDont say anything else, just stick to the format above\"\n",
    "    response = evaluator_gpt(eval_query)\n",
    "    #add the response to the dataframe in the QUAL_EVAL column\n",
    "    idf['QUAL_EVAL'][i] = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:47<00:00,  4.70s/it]\n"
     ]
    }
   ],
   "source": [
    "#generate a more creative version of the prompt\n",
    "for i in tqdm(range(len(idf))):\n",
    "    #get the domain, category, subcategory and subcategory description from the first row of the dataframe\n",
    "    prompt = idf['PROMPT'][i]\n",
    "    eval_query=\"You are now tasked with providing a slightly more creative and more domain depth version of the question. Add 2 more instructions.\\nRespond with only the new question:\\n\" + prompt + \"\\n\\nDont say anything else.\"\n",
    "    response = evaluator_gpt(eval_query)\n",
    "    #add the response to the dataframe in the IMPROVED_PROMPT column\n",
    "    idf['IMPROVED_PROMPT'][i] = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf.to_csv('Amsel_Eval.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
