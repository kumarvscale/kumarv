{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating login request with your identity provider. A browser window should have opened for you to complete the login. If you can't see it, check existing browser windows, or your OS settings. Press CTRL+C to abort and try again...\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "\n",
    "api_key_file = '/Users/vishalkumar/Documents/apikey.txt'\n",
    "if os.path.isfile(api_key_file):\n",
    "    with open(api_key_file) as f:\n",
    "        openai.api_key = f.readline()\n",
    "else:\n",
    "    print(f\"Error: {api_key_file} not found.\")\n",
    "\n",
    "OPENAI_API_KEY = openai.api_key\n",
    "\n",
    "#login to snowflake db\n",
    "con = snowflake.connector.connect(user='vishal.kumar@scale.com',\n",
    "                                 account='pxa65918',\n",
    "                                 authenticator='externalbrowser',\n",
    "                                 warehouse='COMPUTE_WH',\n",
    "                                 database='SCALE_CRAWLER',\n",
    "                                 role='GENERAL_RO')\n",
    "\n",
    "cs = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator_gpt(prompt):\n",
    "    import openai\n",
    "    client = openai\n",
    "    text_response = client.chat.completions.create(\n",
    "        model=\"gpt-4-0125-preview\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=512\n",
    "    )\n",
    "    return text_response\n",
    "\n",
    "def vision_gpt(image_url, prompt):\n",
    "    import openai\n",
    "    client = openai\n",
    "    vision_response = client.chat.completions.create(\n",
    "        model=\"gpt-4-vision-preview\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": image_url,\n",
    "                            \"detail\": \"high\"\n",
    "                        },\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "    return vision_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data from snowflake\n",
    "sql = f'''\n",
    "with alltasks AS (\n",
    "    select\n",
    "        task\n",
    "    FROM\n",
    "        SCALE_PROD.PUBLIC.PIPELINEV3HUMANNODES\n",
    "    WHERE\n",
    "        review_level IN (12,10)\n",
    "        and project = '65e23dd219c580a16e44e374'\n",
    "        and status = 'pending'\n",
    ")\n",
    "select DISTINCT\n",
    "    v.task,\n",
    "    t_prompter.metadata:image_url AS IMAGE,\n",
    "    t_prompter.metadata:image_type AS IMAGE_TYPE,\n",
    "    t_prompter.metadata:pr_type::string AS PR_TYPE,\n",
    "    u_prompter.email AS prompter,\n",
    "    ta_prompter.attempted_by AS prompter_attempt_id,\n",
    "    ta_prompter.response:responses[2]:output::STRING AS PROMPTER_PROMPT,\n",
    "    ta_prompter.response:responses[3]:output::STRING AS PROMPTER_RESPONSE,\n",
    "    u_reviewer.email AS reviewer,\n",
    "    ta_reviewer.attempted_by AS REVIEWER_ATTEMPT_ID,\n",
    "    ta_reviewer.response:responses[2]:output::STRING AS REVIEWER_PROMPT,\n",
    "    ta_reviewer.response:responses[3]:output::STRING AS REVIEWER_RESPONSE,\n",
    "    ta_reviewer.response:responses[5]:context:response:annotations:\"pr-type-validation\":response[0][0]::text as pr_type_validation,\n",
    "FROM\n",
    "    SCALE_PROD.PUBLIC.PIPELINEV3HUMANNODES v\n",
    "    JOIN (SELECT task FROM alltasks) at ON at.task = v.task\n",
    "    JOIN SCALE_PROD.PUBLIC.TASKATTEMPTS ta_prompter ON ta_prompter.task = v.task AND ta_prompter.attempted_at_review_level = -1 AND ta_prompter.review_status = 'fixed'\n",
    "    JOIN SCALE_PROD.PUBLIC.USERS u_prompter ON u_prompter._id = ta_prompter.attempted_by\n",
    "    JOIN SCALE_PROD.PUBLIC.TASKS t_prompter ON t_prompter._id = v.task\n",
    "    JOIN SCALE_PROD.PUBLIC.TASKATTEMPTS ta_reviewer ON ta_reviewer.task = v.task AND ta_reviewer.attempted_at_review_level = 0 AND ta_reviewer.review_outcome = 'fixed'\n",
    "    JOIN SCALE_PROD.PUBLIC.USERS u_reviewer ON u_reviewer._id = ta_reviewer.attempted_by\n",
    "    JOIN SCALE_PROD.PUBLIC.TASKS t_reviewer ON t_reviewer._id = v.task\n",
    "WHERE\n",
    "    v.project IN ('65e23dd219c580a16e44e374')\n",
    "'''\n",
    "cs.execute(sql)\n",
    "idf = cs.fetch_pandas_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_grammar =\"Please check if the given text has any spelling or grammatical errors. ONLY say 'Yes' if there are errors, otherwise ONLY say 'No'. Here is the text:\\n\"\n",
    "prompt_instruction_following =\"I am give you a question and its answer. Check that all the major questions are answered. ONLY say 'Yes' if the answer covers the major parts of the question, otherwise ONLY say 'No' . Here is the question and the answer:\\n\"\n",
    "prompt_factuality =\"Please check if the given text has any major factual errors. ONLY say 'Yes' if there are factual errors, otherwise ONLY say 'No'. Here is the text:\\n\"\n",
    "prompt_answerable=\"I am giving you a question. You need to tell me if you can answer it or not. ONLY say 'Yes' if you can answer, ONLY say 'No' if more information is needed. Here is the question:\\n\"\n",
    "prompt_pr=\"Please check if the given text mentions that the there is some uncertainty in being able to answer the question. If it does, then ONLY say 'Yes', otherwise if the response is completely CERTAIN then ONLY say 'No'. Here is the text:\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 924/924 [26:38<00:00,  1.73s/it]\n"
     ]
    }
   ],
   "source": [
    "#add a column called ERROR_TYPE to idf\n",
    "idf['POSSIBLE_ERRORS'] = \"\"\n",
    "#iterate over each row of idf and use tqdm to show progress\n",
    "for index, row in tqdm(idf.iterrows(), total=len(idf)):\n",
    "    try:\n",
    "        #print(row['REVIEWER_PROMPT'])\n",
    "        #print(row['REVIEWER_RESPONSE'])\n",
    "        if row['REVIEWER_PROMPT'] == \"[Please edit by clicking the pencil icon]\":\n",
    "            idf.at[index, 'POSSIBLE_ERRORS'] += \"Empty Prompt, \"\n",
    "            #print(\"Empty Prompt\")\n",
    "        if evaluator_gpt(prompt_answerable+row['REVIEWER_PROMPT']).choices[0].message.content == \"Yes\":\n",
    "            # concatenate the error with the current value of POSSIBLE_ERRORS\n",
    "            idf.at[index, 'POSSIBLE_ERRORS'] += \"Answerable without image, \"\n",
    "            #print(\"Answerable without image\")\n",
    "        if row['PR_TYPE_VALIDATION'] == \"\":\n",
    "            if row['PR_TYPE'] == \"CERTAIN\":\n",
    "                if evaluator_gpt(prompt_instruction_following+\"Question:\"+row['REVIEWER_PROMPT']+\"\\nAnswer:\"+row['REVIEWER_RESPONSE']).choices[0].message.content == \"No\":\n",
    "                    # concatenate the error with the current value of POSSIBLE_ERRORS\n",
    "                    idf.at[index, 'POSSIBLE_ERRORS'] += \"All questions not answered, \"\n",
    "                    #print(\"All questions not answered\")\n",
    "        else:\n",
    "            if row['PR_TYPE_VALIDATION'] == \"CERTAIN\":\n",
    "                if evaluator_gpt(prompt_instruction_following+\"Question:\"+row['REVIEWER_PROMPT']+\"\\nAnswer:\"+row['REVIEWER_RESPONSE']).choices[0].message.content == \"No\":\n",
    "                    # concatenate the error with the current value of POSSIBLE_ERRORS\n",
    "                    idf.at[index, 'POSSIBLE_ERRORS'] += \"All questions not answered, \"\n",
    "                    #print(\"All questions not answered\")\n",
    "        if evaluator_gpt(prompt_factuality+row['REVIEWER_RESPONSE']).choices[0].message.content == \"Yes\":\n",
    "            # concatenate the error with the current value of POSSIBLE_ERRORS\n",
    "            idf.at[index, 'POSSIBLE_ERRORS'] += \"Factual error, \"\n",
    "            #print(\"Factual error\")\n",
    "        if evaluator_gpt(prompt_grammar+row['REVIEWER_RESPONSE']).choices[0].message.content == \"Yes\":\n",
    "            # concatenate the error with the current value of POSSIBLE_ERRORS\n",
    "            idf.at[index, 'POSSIBLE_ERRORS'] += \"Grammar error, \"\n",
    "            #print(\"Spelling/Grammar error\")\n",
    "        if row['PR_TYPE_VALIDATION'] == \"\":\n",
    "            if row['PR_TYPE'] == \"UNCERTAIN\":\n",
    "                if evaluator_gpt(prompt_pr+\"Question:\"+row['REVIEWER_PROMPT']+\"\\nAnswer:\"+row['REVIEWER_RESPONSE']).choices[0].message.content == \"No\":\n",
    "                    # concatenate the error with the current value of POSSIBLE_ERRORS\n",
    "                    idf.at[index, 'POSSIBLE_ERRORS'] += \"Potential PR mismatch, \"\n",
    "                    #print(\"Potential PR mismatch\")\n",
    "        else:\n",
    "            if row['PR_TYPE_VALIDATION'] == \"UNCERTAIN\":\n",
    "                if evaluator_gpt(prompt_pr+\"Question:\"+row['REVIEWER_PROMPT']+\"\\nAnswer:\"+row['REVIEWER_RESPONSE']).choices[0].message.content == \"No\":\n",
    "                    # concatenate the error with the current value of POSSIBLE_ERRORS\n",
    "                    idf.at[index, 'POSSIBLE_ERRORS'] += \"Potential PR mismatch, \"\n",
    "                    #print(\"Potential PR mismatch\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred at index {index}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf.to_csv('NG_DI_AutoEval_Output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
