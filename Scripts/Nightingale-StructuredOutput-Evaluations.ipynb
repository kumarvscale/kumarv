{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating login request with your identity provider. A browser window should have opened for you to complete the login. If you can't see it, check existing browser windows, or your OS settings. Press CTRL+C to abort and try again...\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import snowflake.connector\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "api_key_file = 'apikey.txt'\n",
    "if os.path.isfile(api_key_file):\n",
    "    with open(api_key_file) as f:\n",
    "        openai.api_key = f.readline()\n",
    "else:\n",
    "    print(f\"Error: {api_key_file} not found.\")\n",
    "\n",
    "OPENAI_API_KEY = openai.api_key\n",
    "\n",
    "#login to snowflake db\n",
    "con = snowflake.connector.connect(user='vishal.kumar@scale.com',\n",
    "                                 account='pxa65918',\n",
    "                                 authenticator='externalbrowser',\n",
    "                                 warehouse='COMPUTE_WH',\n",
    "                                 database='SCALE_CRAWLER',\n",
    "                                 role='GENERAL_RO')\n",
    "\n",
    "cs = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get work logs\n",
    "sql = f'''\n",
    "select\n",
    "  ta.task as TaskID,\n",
    "  t.params:templateVariables:category::string as tasktype,\n",
    "  t.params:templateVariables:archetype::string as req_1,\n",
    "  t.params:templateVariables:archetypeb::string as req_2,\n",
    "  t.params:templateVariables:archetypec::string as req_3,\n",
    "  ta.response:responses[4]:context:response:annotations:structured_output_type:response[0][0]::string as SO_type,\n",
    "  ta.response:prompt::string as Prompt,\n",
    "  ta.response:responses[3]:context:referenceTexts[0]:content::string as ReferenceText,\n",
    "  ta.response:rewrite::string as Response\n",
    "from\n",
    "  SCALE_PROD.PUBLIC.TASKS t\n",
    "  join SCALE_PROD.PUBLIC.PIPELINEV3HUMANNODES pp on pp.task = t._id\n",
    "  join SCALE_PROD.PUBLIC.taskattempts ta on ta.task = t._id\n",
    "where\n",
    "  t.project = '65a6b29f5abfb1b5efd9303a'\n",
    "  and SO_type IS NOT NULL\n",
    "  and pp.status = 'pending'\n",
    "  and pp.review_level IN (10,12)  \n",
    "\n",
    "-- filters the latest taskattempt of the selected task\n",
    "QUALIFY row_number() OVER (PARTITION BY TA.TASK ORDER BY TA.ATTEMPTED_AT DESC) = 1\n",
    "'''\n",
    "cs.execute(sql)\n",
    "df = cs.fetch_pandas_all()\n",
    "df = df.dropna(subset=['SO_TYPE'])\n",
    "df = df[df['SO_TYPE']!='no_structured_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator_gpt(prompt):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4-turbo-preview\",\n",
    "        messages=[{\"role\":\"user\", \"content\":prompt}],\n",
    "        temperature=0.1\n",
    "    )\n",
    "    response = completion.choices[0].message\n",
    "    response = response.content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 336/336 [03:27<00:00,  1.62it/s]\n"
     ]
    }
   ],
   "source": [
    "#evaluate the response for accuracy of structured output\n",
    "so_type=\"\"\n",
    "df['EVALUATION_RESULT'] = ''\n",
    "#start a for loop with tqdm for all rows of df\n",
    "for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    #save value of current row SO_TYPE column in so_type variable\n",
    "    so_type = row['SO_TYPE']\n",
    "    eval_set = row['RESPONSE']\n",
    "    prompt = \"Read this \" + so_type + \" and check whether it is in correct format. Only check the format if it is correct mardown or json or html or csv or table or xml. Do not check accuracy of content. Here it is:\\n\\n\\n\" + eval_set + \"\\n\\n\\n Only respond with Correct if format is correct, else say incorrect. Dont say anything else\"\n",
    "    #print(prompt)\n",
    "    response = evaluator_gpt(prompt)\n",
    "    df.loc[i, 'evaluation_result'] = response\n",
    "    #print(response)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             SO_TYPE  COUNT  PERCENTAGE_CORRECT\n",
      "0                csv     37            0.540541\n",
      "1               html      5            0.000000\n",
      "2               json     44            0.568182\n",
      "3  python_dictionary      7            0.714286\n",
      "4              table    236            0.902542\n",
      "5                xml      7            0.285714\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#copy df into a new dataframe odf\n",
    "odf = df.copy()\n",
    "#replace Correct by 1 in evaluation_result column of odf\n",
    "odf['evaluation_result'] = odf['evaluation_result'].replace('Correct', 1)\n",
    "odf['evaluation_result'] = odf['evaluation_result'].replace('Incorrect', 0)\n",
    "odf['evaluation_result'] = odf['evaluation_result'].replace('incorrect', 0)\n",
    "df['evaluation_result'] = pd.to_numeric(odf['evaluation_result'])\n",
    "#calculate mean of evaluation_result by SO_TYPE and print, also display the count of each SO_TYPE, in the same table\n",
    "# First, group by 'SO_TYPE' and then aggregate to get count and mean for 'evaluation_result'\n",
    "grouped = df.groupby('SO_TYPE')['evaluation_result'].agg(['count', 'mean'])\n",
    "\n",
    "# Reset index to turn the index into a column, making the DataFrame suitable for printing\n",
    "grouped_reset = grouped.reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "grouped_reset.columns = ['SO_TYPE', 'COUNT', 'PERCENTAGE_CORRECT']\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(grouped_reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('structured_output_evaluation.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
