{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "\n",
    "\n",
    "api_key_file = '/Users/vishalkumar/Documents/apikey.txt'\n",
    "if os.path.isfile(api_key_file):\n",
    "    with open(api_key_file) as f:\n",
    "        openai.api_key = f.readline()\n",
    "else:\n",
    "    print(f\"Error: {api_key_file} not found.\")\n",
    "\n",
    "OPENAI_API_KEY = openai.api_key\n",
    "\n",
    "#login to snowflake db\n",
    "con = snowflake.connector.connect(user='vishal.kumar@scale.com',\n",
    "                                 account='pxa65918',\n",
    "                                 authenticator='externalbrowser',\n",
    "                                 warehouse='COMPUTE_WH',\n",
    "                                 database='SCALE_CRAWLER',\n",
    "                                 role='GENERAL_RO')\n",
    "\n",
    "cs = con.cursor()\n",
    "\n",
    "\n",
    "def evaluator_gpt(prompt):\n",
    "    import openai\n",
    "    client = openai\n",
    "    text_response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=512\n",
    "    )\n",
    "    return text_response\n",
    "\n",
    "\n",
    "def ismath(exp):\n",
    "    try:\n",
    "        result = pd.eval(exp)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data from snowflake\n",
    "sql = f'''\n",
    "select\n",
    "  ta._ID,\n",
    "  ta.task,\n",
    "  ta.attempted_by,\n",
    "  ta.response :responses [5].context.candidates [0].message.content :: string as Prompt,\n",
    "  ta.response :responses [5].output :: string as Response,\n",
    "  t.metadata :image_url :: string as image_url\n",
    "from\n",
    "  scale_prod.public.taskattempts ta\n",
    "  join scale_prod.public.tasks t on t._ID = ta.task\n",
    "where\n",
    "  ta.project = '667dbe0f5e08440a357aa3c2'\n",
    "  and ta.attempted_at_review_level = -1\n",
    "limit 40\n",
    "'''\n",
    "cs.execute(sql)\n",
    "idf = cs.fetch_pandas_all()\n",
    "print(idf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA Check: Math calculation accuracy\n",
    "# Step 1: Start a for loop to iterate over the rows of idf\n",
    "# Step 2: For each row, pass the response to evaluator_gpt function and ask to extract math calculations each in a new line\n",
    "# Step 3: Pass each new line to pandas eval function\n",
    "# Step 4: Save the results in a new variable\n",
    "# Step 5: Compare the results with the results in responses using evaluator_gpt\n",
    "# Step 6: Return any incorrect results if found\n",
    "\n",
    "for index, row in tqdm(idf.iterrows(), total=idf.shape[0]):\n",
    "    correct_count = 0\n",
    "    total_math_calculations = 0 \n",
    "    response = row['RESPONSE']\n",
    "    math_prompt = (\"From the provided text, only extract the mathematical calculations such that I can pass them \"\n",
    "                   \"directly to pandas eval function. A mathematical calculation is defined as one with mathematical operators. \"\n",
    "                   \"Do not convert text/string to math operators. If no math calculation is present, just say 'NA'. If math calculation \"\n",
    "                   \"is present, put each calculation in a new line.\\n\")\n",
    "    math_response = evaluator_gpt(math_prompt + response).choices[0].message.content\n",
    "    math_response = math_response.split('\\n')\n",
    "    math_response = [x for x in math_response if x]\n",
    "\n",
    "    if math_response[0] == 'NA':\n",
    "        idf.at[index, 'Math_Response'] = ''\n",
    "        idf.at[index, 'Result'] = ''\n",
    "        idf.at[index, 'Compare_Response'] = ''\n",
    "        idf.at[index, 'Correct_Percentage'] = ''\n",
    "        continue\n",
    "    \n",
    "    result_output = []\n",
    "    compare_output = []\n",
    "\n",
    "    for math in math_response:\n",
    "        if ismath(math):\n",
    "            math = math.replace(',', '') #required to use pandas eval function\n",
    "            result = pd.eval(math)\n",
    "            result = round(result, 3) #specifically left to 3 to allow rounding when percentage is being calculated. Results will vary if this is changed. Recommned leaving at 3\n",
    "            result = \"{:,}\".format(result) #required to compare outputs correctly\n",
    "            result_output.append(result)\n",
    "            compare_prompt = (f\"Given the math calculation: {math}, the result is {result}. Evaluate whether this is the same result appearing in the text provided. \"\n",
    "                              f\"if yes, respond 'Correct' else respond 'Incorrect'\\nWhile comparing the result provided earlier to the result in the text, make sure to \"\n",
    "                              f\"convert any percentages to decimals, or vice versa in order to match the format of the two. Also, allow decimal rounding imprecision. \"\n",
    "                              f\"Text to evaluate:\\n {response}\")\n",
    "            compare_response = evaluator_gpt(compare_prompt).choices[0].message.content\n",
    "            \n",
    "            if compare_response == 'Correct':\n",
    "                correct_count += 1\n",
    "            compare_output.append(compare_response)\n",
    "\n",
    "    total_math_calculations = len(compare_output)\n",
    "\n",
    "    if total_math_calculations > 0:\n",
    "        correct_percentage = correct_count / total_math_calculations\n",
    "    else:\n",
    "        correct_percentage = ''\n",
    "\n",
    "    idf.at[index, 'Math_Response'] = math_response\n",
    "    idf.at[index, 'Result'] = result_output\n",
    "    idf.at[index, 'Compare_Response'] = compare_output\n",
    "    idf.at[index, 'Correct_Percentage'] = correct_percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA Check: Prompt is answerable without image\n",
    "for index, row in tqdm(idf.iterrows(), total=idf.shape[0]):\n",
    "    question = row['PROMPT']\n",
    "    response = evaluator_gpt(row['RESPONSE']).choices[0].message.content\n",
    "    image_prompt = (\"Giving you a question and its response. Are all the questions answered correctly? or more information is asked from the user. Here is the question:\\n{question}\"\n",
    "                    f\"\\n Here is the answer.\\n{response}\\n. Assess whether the answer is complete or more information was needed to provide a complete and correct answer. Dont make assumptions about the question.\"\n",
    "                    f\"Respond 'Complete' if the answer is complete and 'Incomplete' if the answer is incomplete.\\n\")\n",
    "    image_response = evaluator_gpt(image_prompt).choices[0].message.content\n",
    "    print(image_response)\n",
    "    print(row['TASK'])\n",
    "    idf.at[index, 'Image_Response'] = image_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA Check: Auto SOTA Comparison\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
